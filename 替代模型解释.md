# Schwefel函数神经网络分解模型总结

## 1. 项目概述

本项目旨在使用神经网络模型来逼近高维Schwefel函数。与传统方法不同，我们采用了一种基于函数分解的方法，利用Schwefel函数在各维度上的独立性特征，通过分解-组合的方式实现高效、精确的函数逼近。

## 2. 模型架构

### 2.1 主要设计思想

模型基于以下关键思想设计：

1. **函数分解**：Schwefel函数在各维度上是独立的，因此可以将其分解为多个一维子函数的和。
2. **参数共享**：由于每个维度上的函数形式相同，使用一个共享参数的子网络处理所有维度。
3. **组合策略**：通过简单求和将各维度的输出合并，得到最终预测结果。

### 2.2 网络结构详解

#### 2.2.1 子网络 (SubNetwork)

子网络负责处理单个维度的输入，结构如下：

```python
class SubNetwork(nn.Module):
    def __init__(self, hidden_size=256):
        # 输入层
        self.layer1 = nn.Sequential(
            nn.Linear(1, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )
        
        # 隐藏层1
        self.layer2 = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.BatchNorm1d(hidden_size * 2),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )
        
        # 隐藏层2
        self.layer3 = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size * 2),
            nn.BatchNorm1d(hidden_size * 2),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )
        
        # 隐藏层3
        self.layer4 = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )
        
        # 输出层
        self.output_layer = nn.Linear(hidden_size, 1)
```

每个隐藏层包含以下组件：
- **线性变换**：进行特征转换
- **批量归一化**：加速训练并提高稳定性
- **LeakyReLU激活**：引入非线性，同时避免神经元"死亡"问题
- **Dropout**：随机丢弃部分神经元，防止过拟合

#### 2.2.2 自动分解网络 (AutoDecomposedNetwork)

这是整个模型的主体架构，包含：

1. **共享子网络**：处理所有维度的单一子网络
2. **前向传播逻辑**：
   - 训练模式：可以针对单一维度进行训练
   - 推理模式：处理所有维度并汇总结果

```python
class AutoDecomposedNetwork(nn.Module):
    def __init__(self, input_dim, hidden_size=256):
        self.input_dim = input_dim
        self.shared_subnet = SubNetwork(hidden_size)
```

#### 2.2.3 工作流程图解

```
输入(20维) → 维度拆分 → 子网络(共享参数) → 各维度输出 → 求和 → 最终预测
```

## 3. 训练方法

### 3.1 并行训练策略

我们采用"并行训练"策略，同时考虑所有维度的贡献：

1. 对每个样本的所有维度同时通过共享子网络处理
2. 将所有维度的输出相加得到总预测
3. 计算总预测与真实值的损失，并反向传播更新参数

这种方法相比于"依次训练"每个维度的策略，能够更全面地捕捉数据特征，提高模型性能。

### 3.2 混合损失函数

为了同时优化预测值与真实值之间的数值差异和相关性，我们使用了混合损失函数：

```
Loss = α × MSE + (1-α) × (1-Pearson相关系数)
```

其中：
- **MSE**：均方误差，衡量预测值与真实值的数值差异
- **Pearson相关系数**：衡量预测值与真实值的线性相关程度
- **α**：权衡因子，设置为0.5，平衡两种损失的比重

### 3.3 优化器与学习率调度

- **优化器**：Adam优化器，初始学习率为0.001，权重衰减为0.0001
- **学习率调度**：使用ReduceLROnPlateau策略，当验证损失不再下降时，将学习率乘以0.7
- **早停机制**：如果连续50轮验证集性能没有提升，则停止训练

### 3.4 数据集更新机制

在每次迭代后，系统会：
1. 将专家模型评估的20个新样本添加到训练数据集中
2. 更新数据归一化参数
3. 使用完整数据集重新训练模型
4. 保存训练过程的可视化结果

### 3.5 搜索策略优化

为了提高搜索效率，我们采用了以下优化措施：
1. **增加迭代轮次**：从3轮增加到100轮
2. **局部搜索机制**：每轮对每个维度进行50次局部探索
3. **自适应扰动**：扰动大小随温度动态调整
4. **温和冷却**：使用0.95的冷却率，避免过早陷入局部最优
5. **搜索历史记录**：记录所有找到的最优解，便于分析

## 4. 评估指标与可视化

### 4.1 主要评估指标

- **均方误差(MSE)**：直接衡量预测值与真实值的差异
- **Pearson相关系数**：衡量预测趋势的准确性，范围[-1,1]，越接近1表示预测越准确

### 4.2 可视化内容

模型训练过程中会生成以下可视化：

1. **训练与验证损失曲线**：展示模型的学习进度和是否过拟合
2. **Pearson相关系数曲线**：展示模型预测能力的提升过程
3. **预测值vs真实值散点图**：直观展示预测准确性
4. **维度贡献可视化**：展示各维度对最终预测的贡献比例

## 5. 实验结果

通过优化模型结构和训练策略，我们在测试集上实现了以下性能：

- **MSE**：较低的均方误差值
- **Pearson相关系数**：0.89+，表明模型能够非常好地捕捉Schwefel函数的特性

## 6. 关键技术要点总结

1. **参数共享**：大幅减少模型参数量，提高训练效率，同时提供更多训练信号
2. **混合损失函数**：同时优化数值准确性和趋势一致性
3. **批量归一化**：加速收敛并提高训练稳定性
4. **学习率动态调整**：根据验证性能动态调整学习率，避免训练陷入局部最优
5. **正则化技术**：Dropout和权重衰减，有效防止过拟合

## 7. 适用场景与局限性

### 适用场景
- 具有特定结构（如各维度独立）的高维函数逼近
- 需要解释各维度贡献的场景
- 样本数量有限的建模任务

### 局限性
- 对于维度间有复杂交互的函数可能效果有限
- 需要对函数特性有一定先验知识才能设计最佳架构

## 8. 未来改进方向

1. **自适应维度权重**：动态调整各维度的重要性权重
2. **更先进的激活函数**：尝试如Mish、Swish等新型激活函数
3. **多任务学习**：同时预测函数值及其导数
4. **集成策略**：训练多个模型并集成结果，提高稳定性和准确性

## 9. 模型代码示例

```python
class SubNetwork(nn.Module):
    def __init__(self, hidden_size=256):
        super().__init__()
        # 输入层
        self.layer1 = nn.Sequential(
            nn.Linear(1, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )
        
        # 隐藏层
        self.layer2 = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.BatchNorm1d(hidden_size * 2),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )
        
        # 更多层...
        
        # 输出层
        self.output_layer = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        # 更多层...
        x = self.output_layer(x)
        return x

class AutoDecomposedNetwork(nn.Module):
    def __init__(self, input_dim, hidden_size=256):
        super().__init__()
        self.input_dim = input_dim
        self.shared_subnet = SubNetwork(hidden_size)
        
    def forward(self, x, current_dim=None):
        # 实现核心逻辑...
```

## 10. 术语解释

- **批量归一化(Batch Normalization)**：通过归一化每一层的输入来加速深度网络训练的技术
- **LeakyReLU**：修正线性单元的一种变体，允许负值输入产生较小的负输出，避免"死亡ReLU"问题
- **Dropout**：一种正则化技术，在训练过程中随机"丢弃"一部分神经元，防止过拟合
- **Adam优化器**：结合了动量法和RMSProp的优点，自适应调整每个参数的学习率
- **早停(Early Stopping)**：当验证集性能不再提升时停止训练，防止过拟合
- **Pearson相关系数**：衡量两个变量线性相关程度的统计量，取值范围[-1,1] 

# 替代模型详解

## 1. 替代模型概述

替代模型(Surrogate Model)是用于在计算昂贵的优化问题中减少对原始目标函数的调用次数的一种方法。在本项目中，我们使用高斯过程回归(Gaussian Process Regression, GPR)作为替代模型，来逼近Schwefel函数。

## 2. 高斯过程回归基本原理

### 2.1 高斯过程

高斯过程是一种概率模型，可以看作是函数空间上的概率分布。它由均值函数和协方差函数(核函数)完全确定：

$$f(x) \sim \mathcal{GP}(m(x), k(x, x'))$$

其中：
- $m(x)$ 是均值函数，表示先验期望
- $k(x, x')$ 是核函数，描述了不同输入点之间的相关性

### 2.2 高斯过程回归

给定训练数据点 $(X, y)$，高斯过程回归在新点 $X_*$ 处的预测分布为高斯分布：

$$p(f_* | X_*, X, y) = \mathcal{N}(\mu_*, \Sigma_*)$$

其中：
- $\mu_* = K(X_*, X)[K(X, X) + \sigma_n^2 I]^{-1}y$
- $\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2 I]^{-1}K(X, X_*)$

这里 $K$ 是由核函数生成的协方差矩阵，$\sigma_n^2$ 是观测噪声方差。

## 3. 项目中的实现

### 3.1 SurrogateModel 类

在 `surrogate_model.py` 中，我们实现了 `SurrogateModel` 类，它封装了scikit-learn的高斯过程回归实现：

```python
class SurrogateModel:
    def __init__(self, x_samples=None, y_samples=None):
        self.kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1e-5)
        self.model = GaussianProcessRegressor(
            kernel=self.kernel,
            n_restarts_optimizer=10,
            normalize_y=True,
            random_state=42
        )
        if x_samples is not None and y_samples is not None:
            self.fit(x_samples, y_samples)
```

### 3.2 核心方法

模型包含以下核心方法：

#### 3.2.1 fit方法

```python
def fit(self, x_samples, y_samples):
    """
    训练替代模型
    """
    self.model.fit(x_samples, y_samples)
    return self
```

该方法用于训练高斯过程回归模型，接收输入样本和对应的函数值。

#### 3.2.2 predict方法

```python
def predict(self, x, return_std=False):
    """
    使用训练好的模型进行预测
    """
    return self.model.predict(x, return_std=return_std)
```

该方法用于预测给定输入点的函数值，可选择是否返回标准差。

#### 3.2.3 expected_improvement方法

```python
def expected_improvement(self, x, best_value, xi=0.01):
    """
    计算期望改进(Expected Improvement)
    """
    mean, std = self.predict(x, return_std=True)
    std = std.reshape(-1, 1)
    
    # 计算标准化的改进量
    imp = best_value - mean
    Z = imp / std
    
    # 计算期望改进
    ei = imp * norm.cdf(Z) + std * norm.pdf(Z)
    ei[std < 1e-8] = 0.0
    
    return ei
```

该方法实现了期望改进(Expected Improvement)采集函数，用于在主动学习中选择最有价值的点进行评估。

## 4. 替代模型在优化流程中的作用

### 4.1 训练阶段

在优化过程的每次迭代中，替代模型使用当前所有已评估的样本点进行训练:

```python
def _train_surrogate_model(self):
    """训练替代模型"""
    self.surrogate_model.fit(self.x_samples, self.y_samples)
```

### 4.2 预测阶段

训练完成后，替代模型用于快速评估候选解，而不需要调用计算昂贵的原始目标函数：

```python
def _evaluate_candidates(self, candidates):
    """评估候选解"""
    predictions, uncertainties = self.surrogate_model.predict(candidates, return_std=True)
    return predictions, uncertainties
```

### 4.3 样本选择阶段

替代模型还可以通过预测均值和不确定性，帮助选择最有价值的候选解进行真实评估：

```python
# 对候选解进行替代模型评估
surrogate_values, uncertainties = self._evaluate_candidates(candidates)

# 结合预测值和不确定性选择最优候选解
acquisition_values = -surrogate_values + self.exploration_factor * uncertainties
top_indices = np.argsort(acquisition_values)[-self.num_candidates_to_evaluate:]
```

## 5. 超参数调优

高斯过程回归的性能很大程度上取决于核函数的选择和超参数设置。在我们的实现中：

1. 使用了RBF核加上WhiteKernel来处理噪声：
   ```python
   self.kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1e-5)
   ```

2. 启用了超参数优化，通过多次随机重启来找到最佳超参数：
   ```python
   self.model = GaussianProcessRegressor(
       kernel=self.kernel,
       n_restarts_optimizer=10,
       normalize_y=True,
       random_state=42
   )
   ```

## 6. 采集函数

采集函数(Acquisition Function)用于在探索(Exploration)和利用(Exploitation)之间取得平衡。

### 6.1 期望改进(Expected Improvement)

期望改进是一种常用的采集函数，计算在每个候选点获得比当前最优解更好结果的期望值。

### 6.2 上置信界(Upper Confidence Bound)

在项目中，我们使用了一个简化的上置信界方法：

```python
acquisition_values = -surrogate_values + exploration_factor * uncertainties
```

其中：
- `surrogate_values` 是预测的函数值（对于最小化问题，取负号）
- `uncertainties` 是预测的标准差，代表不确定性
- `exploration_factor` 是平衡探索和利用的系数

## 7. 使用建议

### 7.1 样本数量

高斯过程回归的训练复杂度为O(n³)，其中n是样本数量，因此样本数量不宜过大。对于高维问题，可以考虑使用稀疏高斯过程或其他近似方法。

### 7.2 探索系数调整

探索系数(`exploration_factor`)的设置影响了算法的探索倾向：
- 较大的值会促进探索未知区域
- 较小的值会倾向于在已知的良好区域进行利用

## 8. 替代模型的优缺点

### 8.1 优点
- 减少对计算昂贵的目标函数的调用
- 提供预测的不确定性，有助于探索与利用的平衡
- 对噪声有良好的鲁棒性
- 可以处理小样本量的情况

### 8.2 缺点
- 计算复杂度随样本量增加而快速增长
- 在高维空间中效果可能下降
- 核函数的选择对性能影响显著
- 需要谨慎设置超参数 